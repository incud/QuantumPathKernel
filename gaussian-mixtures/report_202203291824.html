
<html>
    <head>
        <title>Gaussian Mixtures with Quantum Machine Learning models and Path Kernel</title>
        <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-1BmE4kWBq78iYhFldvKuhfTAU6auU8tT94WrHftjDbrCEXSU1oBoqyl2QvZ6jIW3" crossorigin="anonymous">
        <style>img{max-width: 600px}</style>
    </head>
    <body class="container">
        <h1>Gaussian Mixtures with Quantum Machine Learning models and Path Kernel</h1>
        <p>Generated at 29/03/2022 18:20:38</p>
        <h2>Dataset</h2>
        <p>The dataset is composed of N samples. Each sample has D components in the form (x1, x2, 0, 0, ..., 0)
        where x1 and x2 are the coordinated of one of the four centroids (+-.5, +-.5) plus some noise that I've called
        snr (signal to noise) but I'm not sure it matches the definition... Is just noise independently sampled from
        a gaussian having mean zero and variance equal to the 'snr'. The higher the 'snr', the more noisy my dataset
        is and the more difficult is to classify the samples. 
        In Refinetti's work, having large value to D results in a difficult environment for random feature kernel models
        while Neural Networks, due to their dissipative work, can easily learn the distribution reaching the optimal
        performance of the oracle. </p>
        <p>Dataset generated with snr=0.10:</br><img src='experiment_snr0.10_d4_lbce_202203261440/analysis/dataset_plot.png'/></p>
<p>Dataset generated with snr=0.10:</br><img src='experiment_snr0.10_d4_lmse_202203241830/analysis/dataset_plot.png'/></p>
<p>Dataset generated with snr=0.10:</br><img src='experiment_snr0.10_d6_lmse_202203250943/analysis/dataset_plot.png'/></p>
<p>Dataset generated with snr=0.20:</br><img src='experiment_snr0.20_d5_lmse_202203280951/analysis/dataset_plot.png'/></p>
<p>Dataset generated with snr=0.20:</br><img src='experiment_snr0.20_d6_lmse_202203262101/analysis/dataset_plot.png'/></p>
<p>Dataset generated with snr=0.40:</br><img src='experiment_snr0.40_d4_lbce_202203282128/analysis/dataset_plot.png'/></p>
<p>Dataset generated with snr=0.50:</br><img src='experiment_snr0.50_d5_lbce_202203290759/analysis/dataset_plot.png'/></p>
<p>Dataset generated with snr=0.60:</br><img src='experiment_snr0.60_d4_lbce_202203282128/analysis/dataset_plot.png'/></p>
<p>Dataset generated with snr=0.80:</br><img src='experiment_snr0.80_d4_lbce_202203271938/analysis/dataset_plot.png'/></p>
<p>Dataset generated with snr=0.80:</br><img src='experiment_snr0.80_d5_lbce_202203280956/analysis/dataset_plot.png'/></p>
<p>Dataset generated with snr=0.80:</br><img src='experiment_snr0.80_d5_lmse_202203290758/analysis/dataset_plot.png'/></p>
<p>Dataset generated with snr=1.00:</br><img src='experiment_snr1.00_d4_lbce_202203261741/analysis/dataset_plot.png'/></p>
<p>Dataset generated with snr=1.00:</br><img src='experiment_snr1.00_d4_lmse_202203271937/analysis/dataset_plot.png'/></p>

        </br>
        <h2>Loss</h2>
        <p>The loss is the function minimized during the training phase by the optimizer. In classical deep learning 
        theory, I should reach zero loss when I have just enough parameters to perfectly fit the data, resulting in 
        a large generalization error. Due to double descent and the implic regularization of gradient descent 
        optimization, adding further parameters still find a solution having zero loss which although represent a 
        simpler function, with better generalization performances.<p>
        <p>The loss function we can study is either the Binary Cross Entropy, which is used for classification 
        problems such this one, and the Mean Square Error, which is used in regression problems usually but it still
        make sense to make the comparison.</p>
        <h4>Loss per epoch</h2>
        <p>The following paragram study the loss of each model with respect to the epoch of training (x axis). The
        color of the line represents how many layer the QNN has.</p>
        <h6>Using loss BCE (Binary Cross Entropy)</h4>
        <p>Experiment having snr=0.10, d=4, loss=bce:</br><img src='experiment_snr0.10_d4_lbce_202203261440/analysis/loss_in_training_per_epoch.png'/></p>
<p>Experiment having snr=0.40, d=4, loss=bce:</br><img src='experiment_snr0.40_d4_lbce_202203282128/analysis/loss_in_training_per_epoch.png'/></p>
<p>Experiment having snr=0.50, d=5, loss=bce:</br><img src='experiment_snr0.50_d5_lbce_202203290759/analysis/loss_in_training_per_epoch.png'/></p>
<p>Experiment having snr=0.60, d=4, loss=bce:</br><img src='experiment_snr0.60_d4_lbce_202203282128/analysis/loss_in_training_per_epoch.png'/></p>
<p>Experiment having snr=0.80, d=4, loss=bce:</br><img src='experiment_snr0.80_d4_lbce_202203271938/analysis/loss_in_training_per_epoch.png'/></p>
<p>Experiment having snr=0.80, d=5, loss=bce:</br><img src='experiment_snr0.80_d5_lbce_202203280956/analysis/loss_in_training_per_epoch.png'/></p>
<p>Experiment having snr=1.00, d=4, loss=bce:</br><img src='experiment_snr1.00_d4_lbce_202203261741/analysis/loss_in_training_per_epoch.png'/></p>

        <h6>Using loss MSE (Mean Square Error)</h6>
        <p>Experiment having snr=0.10, d=4, loss=mse:</br><img src='experiment_snr0.10_d4_lmse_202203241830/analysis/loss_in_training_per_epoch.png'/></p>
<p>Experiment having snr=0.10, d=6, loss=mse:</br><img src='experiment_snr0.10_d6_lmse_202203250943/analysis/loss_in_training_per_epoch.png'/></p>
<p>Experiment having snr=0.20, d=5, loss=mse:</br><img src='experiment_snr0.20_d5_lmse_202203280951/analysis/loss_in_training_per_epoch.png'/></p>
<p>Experiment having snr=0.20, d=6, loss=mse:</br><img src='experiment_snr0.20_d6_lmse_202203262101/analysis/loss_in_training_per_epoch.png'/></p>
<p>Experiment having snr=0.80, d=5, loss=mse:</br><img src='experiment_snr0.80_d5_lmse_202203290758/analysis/loss_in_training_per_epoch.png'/></p>
<p>Experiment having snr=1.00, d=4, loss=mse:</br><img src='experiment_snr1.00_d4_lmse_202203271937/analysis/loss_in_training_per_epoch.png'/></p>

        </br>
        <h4>Loss per depth</h4>
        <p>The following paragram study the loss of each model with respect to the depth (x axis). The point on each 
        vertical line represents the evolution of the loss at a certain depth, during the training each epoch multiple
        of 100. The fact that the points are not evenly spread from the top to the bottom means that the loss 
        immediately reach zero (well, in 100 epoch at least).</p>
        <h6>Using loss BCE (Binary Cross Entropy)</h6>
        <p>Experiment having snr=0.10, d=4, loss=bce:</br><img src='experiment_snr0.10_d4_lbce_202203261440/analysis/loss_in_training_per_depth.png'/></p>
<p>Experiment having snr=0.40, d=4, loss=bce:</br><img src='experiment_snr0.40_d4_lbce_202203282128/analysis/loss_in_training_per_depth.png'/></p>
<p>Experiment having snr=0.50, d=5, loss=bce:</br><img src='experiment_snr0.50_d5_lbce_202203290759/analysis/loss_in_training_per_depth.png'/></p>
<p>Experiment having snr=0.60, d=4, loss=bce:</br><img src='experiment_snr0.60_d4_lbce_202203282128/analysis/loss_in_training_per_depth.png'/></p>
<p>Experiment having snr=0.80, d=4, loss=bce:</br><img src='experiment_snr0.80_d4_lbce_202203271938/analysis/loss_in_training_per_depth.png'/></p>
<p>Experiment having snr=0.80, d=5, loss=bce:</br><img src='experiment_snr0.80_d5_lbce_202203280956/analysis/loss_in_training_per_depth.png'/></p>
<p>Experiment having snr=1.00, d=4, loss=bce:</br><img src='experiment_snr1.00_d4_lbce_202203261741/analysis/loss_in_training_per_depth.png'/></p>

        <h6>Using loss MSE (Mean Square Error)</h6>
        <p>Experiment having snr=0.10, d=4, loss=mse:</br><img src='experiment_snr0.10_d4_lmse_202203241830/analysis/loss_in_training_per_depth.png'/></p>
<p>Experiment having snr=0.10, d=6, loss=mse:</br><img src='experiment_snr0.10_d6_lmse_202203250943/analysis/loss_in_training_per_depth.png'/></p>
<p>Experiment having snr=0.20, d=5, loss=mse:</br><img src='experiment_snr0.20_d5_lmse_202203280951/analysis/loss_in_training_per_depth.png'/></p>
<p>Experiment having snr=0.20, d=6, loss=mse:</br><img src='experiment_snr0.20_d6_lmse_202203262101/analysis/loss_in_training_per_depth.png'/></p>
<p>Experiment having snr=0.80, d=5, loss=mse:</br><img src='experiment_snr0.80_d5_lmse_202203290758/analysis/loss_in_training_per_depth.png'/></p>
<p>Experiment having snr=1.00, d=4, loss=mse:</br><img src='experiment_snr1.00_d4_lmse_202203271937/analysis/loss_in_training_per_depth.png'/></p>

        </br>
        <h2>Parameters norm change</h2>
        <p>Parameters norm change highlight the lazy training phenomena: if the parameters stay close to their 
        initialization then we are in a lazy training regime (a sort of, since our QNN are indeed linear model, 
        can be really call them feature learning vs lazy regimes? isn't that just optimization?).
        <h4>Parameters norm change per epoch</h4>
        <p>These plots highlight the fact that after a (big or small) adjustment of the parameters, they converges 
        quickly into a solution (n.b. note that here just the norm is checked, eventually if they are rotating they
        will indeed change if I can't see that from this plot... It's more a technical note though). 
        <h6>Using loss BCE (Binary Cross Entropy)</h6>
        <p>Experiment having snr=0.10, d=4, loss=bce:</br><img src='experiment_snr0.10_d4_lbce_202203261440/analysis/param_norm_change_in_training_per_epoch.png'/></p>
<p>Experiment having snr=0.40, d=4, loss=bce:</br><img src='experiment_snr0.40_d4_lbce_202203282128/analysis/param_norm_change_in_training_per_epoch.png'/></p>
<p>Experiment having snr=0.50, d=5, loss=bce:</br><img src='experiment_snr0.50_d5_lbce_202203290759/analysis/param_norm_change_in_training_per_epoch.png'/></p>
<p>Experiment having snr=0.60, d=4, loss=bce:</br><img src='experiment_snr0.60_d4_lbce_202203282128/analysis/param_norm_change_in_training_per_epoch.png'/></p>
<p>Experiment having snr=0.80, d=4, loss=bce:</br><img src='experiment_snr0.80_d4_lbce_202203271938/analysis/param_norm_change_in_training_per_epoch.png'/></p>
<p>Experiment having snr=0.80, d=5, loss=bce:</br><img src='experiment_snr0.80_d5_lbce_202203280956/analysis/param_norm_change_in_training_per_epoch.png'/></p>
<p>Experiment having snr=1.00, d=4, loss=bce:</br><img src='experiment_snr1.00_d4_lbce_202203261741/analysis/param_norm_change_in_training_per_epoch.png'/></p>

        <h6>Using loss MSE (Mean Square Error)</h6>
        <p>Experiment having snr=0.10, d=4, loss=mse:</br><img src='experiment_snr0.10_d4_lmse_202203241830/analysis/param_norm_change_in_training_per_epoch.png'/></p>
<p>Experiment having snr=0.10, d=6, loss=mse:</br><img src='experiment_snr0.10_d6_lmse_202203250943/analysis/param_norm_change_in_training_per_epoch.png'/></p>
<p>Experiment having snr=0.20, d=5, loss=mse:</br><img src='experiment_snr0.20_d5_lmse_202203280951/analysis/param_norm_change_in_training_per_epoch.png'/></p>
<p>Experiment having snr=0.20, d=6, loss=mse:</br><img src='experiment_snr0.20_d6_lmse_202203262101/analysis/param_norm_change_in_training_per_epoch.png'/></p>
<p>Experiment having snr=0.80, d=5, loss=mse:</br><img src='experiment_snr0.80_d5_lmse_202203290758/analysis/param_norm_change_in_training_per_epoch.png'/></p>
<p>Experiment having snr=1.00, d=4, loss=mse:</br><img src='experiment_snr1.00_d4_lmse_202203271937/analysis/param_norm_change_in_training_per_epoch.png'/></p>

        </br>
        <h4>Parameters norm change per depth</h4>
        <p>If the yellow-er dots stay close to the red (initial) ones then we are in lazy training. It almost never
        happen than the norm first increase (going far from the initialization) then decreases (returning back to the
        initialization).
        <h6>Using loss BCE (Binary Cross Entropy)</h6>
        <p>Experiment having snr=0.10, d=4, loss=bce:</br><img src='experiment_snr0.10_d4_lbce_202203261440/analysis/param_norm_change_in_training_per_depth.png'/></p>
<p>Experiment having snr=0.40, d=4, loss=bce:</br><img src='experiment_snr0.40_d4_lbce_202203282128/analysis/param_norm_change_in_training_per_depth.png'/></p>
<p>Experiment having snr=0.50, d=5, loss=bce:</br><img src='experiment_snr0.50_d5_lbce_202203290759/analysis/param_norm_change_in_training_per_depth.png'/></p>
<p>Experiment having snr=0.60, d=4, loss=bce:</br><img src='experiment_snr0.60_d4_lbce_202203282128/analysis/param_norm_change_in_training_per_depth.png'/></p>
<p>Experiment having snr=0.80, d=4, loss=bce:</br><img src='experiment_snr0.80_d4_lbce_202203271938/analysis/param_norm_change_in_training_per_depth.png'/></p>
<p>Experiment having snr=0.80, d=5, loss=bce:</br><img src='experiment_snr0.80_d5_lbce_202203280956/analysis/param_norm_change_in_training_per_depth.png'/></p>
<p>Experiment having snr=1.00, d=4, loss=bce:</br><img src='experiment_snr1.00_d4_lbce_202203261741/analysis/param_norm_change_in_training_per_depth.png'/></p>

        <h6>Using loss MSE (Mean Square Error)</h6>
        <p>Experiment having snr=0.10, d=4, loss=mse:</br><img src='experiment_snr0.10_d4_lmse_202203241830/analysis/param_norm_change_in_training_per_depth.png'/></p>
<p>Experiment having snr=0.10, d=6, loss=mse:</br><img src='experiment_snr0.10_d6_lmse_202203250943/analysis/param_norm_change_in_training_per_depth.png'/></p>
<p>Experiment having snr=0.20, d=5, loss=mse:</br><img src='experiment_snr0.20_d5_lmse_202203280951/analysis/param_norm_change_in_training_per_depth.png'/></p>
<p>Experiment having snr=0.20, d=6, loss=mse:</br><img src='experiment_snr0.20_d6_lmse_202203262101/analysis/param_norm_change_in_training_per_depth.png'/></p>
<p>Experiment having snr=0.80, d=5, loss=mse:</br><img src='experiment_snr0.80_d5_lmse_202203290758/analysis/param_norm_change_in_training_per_depth.png'/></p>
<p>Experiment having snr=1.00, d=4, loss=mse:</br><img src='experiment_snr1.00_d4_lmse_202203271937/analysis/param_norm_change_in_training_per_depth.png'/></p>

        </br>
        <h2>Accuracy</h2>
        <p>We compare the accuracy of the model in predicting the labels by comparing the NTK calculated (for each QNN)
        at its last configuration, at the end of the training (1000 epoch) and the Path Kernel. 
        The red-to-yellow line and dots are related to the Neural Tangent Kernel (1000 epoch) + SVM model.
        The blue-to-green line and dots are related to the Path Kernel + SVM model. </p>
        <p>Accuracy is calculated by still using the same dataset of the training (i.e. if I use the non-linearized
        variational model instead of quantum kernel+SVM I will get zero error due to the zero loss). Considering a 
        testing set after this preliminary results is mandatory. 
        <h4>Accuracy per epoch</h4>
        <p>We surely expect the accuracy is higher for small snr. <b>Preliminary thoughts</b>: it seems that with
        small depths the NTK performs better, and with higher depths the NTK performs worse. I was expecting the
        opposite, since the PK at small depth allows to interpret the model as a kernel machine while the NTK at small
        depth (and thus larger parameters norm change) means nothing. We need to check it here something. Well, to tell
        the truth, it seems to me that the performances of PK are almost the average of the performance of NTK...</br>
        P.S: this graph are not great. Do we know a fancier graphical representation?
        <h6>Using loss BCE (Binary Cross Entropy)</h6>
        <p>Experiment having snr=0.10, d=4, loss=bce:</br><img src='experiment_snr0.10_d4_lbce_202203261440/analysis/accuracy_in_training_per_epoch.png'/></p>
<p>Experiment having snr=0.40, d=4, loss=bce:</br><img src='experiment_snr0.40_d4_lbce_202203282128/analysis/accuracy_in_training_per_epoch.png'/></p>
<p>Experiment having snr=0.50, d=5, loss=bce:</br><img src='experiment_snr0.50_d5_lbce_202203290759/analysis/accuracy_in_training_per_epoch.png'/></p>
<p>Experiment having snr=0.60, d=4, loss=bce:</br><img src='experiment_snr0.60_d4_lbce_202203282128/analysis/accuracy_in_training_per_epoch.png'/></p>
<p>Experiment having snr=0.80, d=4, loss=bce:</br><img src='experiment_snr0.80_d4_lbce_202203271938/analysis/accuracy_in_training_per_epoch.png'/></p>
<p>Experiment having snr=0.80, d=5, loss=bce:</br><img src='experiment_snr0.80_d5_lbce_202203280956/analysis/accuracy_in_training_per_epoch.png'/></p>
<p>Experiment having snr=1.00, d=4, loss=bce:</br><img src='experiment_snr1.00_d4_lbce_202203261741/analysis/accuracy_in_training_per_epoch.png'/></p>

        <h6>Using loss MSE (Mean Square Error)</h6>
        <p>Experiment having snr=0.10, d=4, loss=mse:</br><img src='experiment_snr0.10_d4_lmse_202203241830/analysis/accuracy_in_training_per_epoch.png'/></p>
<p>Experiment having snr=0.10, d=6, loss=mse:</br><img src='experiment_snr0.10_d6_lmse_202203250943/analysis/accuracy_in_training_per_epoch.png'/></p>
<p>Experiment having snr=0.20, d=5, loss=mse:</br><img src='experiment_snr0.20_d5_lmse_202203280951/analysis/accuracy_in_training_per_epoch.png'/></p>
<p>Experiment having snr=0.20, d=6, loss=mse:</br><img src='experiment_snr0.20_d6_lmse_202203262101/analysis/accuracy_in_training_per_epoch.png'/></p>
<p>Experiment having snr=0.80, d=5, loss=mse:</br><img src='experiment_snr0.80_d5_lmse_202203290758/analysis/accuracy_in_training_per_epoch.png'/></p>
<p>Experiment having snr=1.00, d=4, loss=mse:</br><img src='experiment_snr1.00_d4_lmse_202203271937/analysis/accuracy_in_training_per_epoch.png'/></p>

        </br>
        <h4>Accuracy per depth</h4>
        <p>Much clearer representation... From these graphs, PK seems to win in general, especially with BCE loss! 
        Still, we are using the same points of the training.</p>
        <h6>Using loss BCE (Binary Cross Entropy)</h6>
        <p>Experiment having snr=0.10, d=4, loss=bce:</br><img src='experiment_snr0.10_d4_lbce_202203261440/analysis/accuracy_in_training_per_depth.png'/></p>
<p>Experiment having snr=0.40, d=4, loss=bce:</br><img src='experiment_snr0.40_d4_lbce_202203282128/analysis/accuracy_in_training_per_depth.png'/></p>
<p>Experiment having snr=0.50, d=5, loss=bce:</br><img src='experiment_snr0.50_d5_lbce_202203290759/analysis/accuracy_in_training_per_depth.png'/></p>
<p>Experiment having snr=0.60, d=4, loss=bce:</br><img src='experiment_snr0.60_d4_lbce_202203282128/analysis/accuracy_in_training_per_depth.png'/></p>
<p>Experiment having snr=0.80, d=4, loss=bce:</br><img src='experiment_snr0.80_d4_lbce_202203271938/analysis/accuracy_in_training_per_depth.png'/></p>
<p>Experiment having snr=0.80, d=5, loss=bce:</br><img src='experiment_snr0.80_d5_lbce_202203280956/analysis/accuracy_in_training_per_depth.png'/></p>
<p>Experiment having snr=1.00, d=4, loss=bce:</br><img src='experiment_snr1.00_d4_lbce_202203261741/analysis/accuracy_in_training_per_depth.png'/></p>

        <h6>Using loss MSE (Mean Square Error)</h6>
        <p>Experiment having snr=0.10, d=4, loss=mse:</br><img src='experiment_snr0.10_d4_lmse_202203241830/analysis/accuracy_in_training_per_depth.png'/></p>
<p>Experiment having snr=0.10, d=6, loss=mse:</br><img src='experiment_snr0.10_d6_lmse_202203250943/analysis/accuracy_in_training_per_depth.png'/></p>
<p>Experiment having snr=0.20, d=5, loss=mse:</br><img src='experiment_snr0.20_d5_lmse_202203280951/analysis/accuracy_in_training_per_depth.png'/></p>
<p>Experiment having snr=0.20, d=6, loss=mse:</br><img src='experiment_snr0.20_d6_lmse_202203262101/analysis/accuracy_in_training_per_depth.png'/></p>
<p>Experiment having snr=0.80, d=5, loss=mse:</br><img src='experiment_snr0.80_d5_lmse_202203290758/analysis/accuracy_in_training_per_depth.png'/></p>
<p>Experiment having snr=1.00, d=4, loss=mse:</br><img src='experiment_snr1.00_d4_lmse_202203271937/analysis/accuracy_in_training_per_depth.png'/></p>

        </br>
        <h2>By increasing D...</h2>
        <p>Does it happens that, like in Refinetti's work, by increasing D the kernel machine loses performances? 
        If that happens, it might mean that the kernel are working just like random features. We still miss some further
        experiments to see a pattern, however:</p>
        <p>Take loss=MSE, snr=0.20, D=5..6</p>
        <p>TODO</p>
        </br></br></br><p>End of report</p></br></br></br>
    </body>
</html>
    
